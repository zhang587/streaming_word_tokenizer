# streaming_word_tokenizer
The goal of this project is to tokenzie 1GB file and get top n of n-grams within one minute.
The first step is to write in python using generator to avoid text materialization. 
The next step is to try to write in cpython and C to make it even faster.
